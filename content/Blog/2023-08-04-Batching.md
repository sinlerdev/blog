<!-- title: Why Batch Processing Matters -->
Let's assume you are sinlernick, and you are working on a new optimization trick for your State management library. 

Before diving into the solution, let's first understand the world-ending problem.

## The Problem
Let's assume you got a Compute object that is computing something extremely expensive. An example like this will do:
```lua
    -- Niche details related to Vinum are omitted.
    local input1 = Hold(LastPosition)
    local input2 = Hold(CurrentPosition)

    local expensiveComputation = Compute(function(use)
        -- This is some very expensive computation!
        return findDistance(use(input1), use(input2))
    end)
```

There aren't any performance issues with the example with relation to Vinum's internals. Everything should be fine! Well, everything is technically fine, until of course, you need to update both `input1` and `input2`.

This is where Vinum tells you "Its not my problem pal!", and forces you to have this unoptimized mess.

## The Chosen One: Solution
Of course, since you are Sinlernick, you are probably sitting on your couch and thinking "How to optimize this". 

Suddenly, an idea enters your brain; What if we allow users to batch write their objects? 

Of course, batching processing will be useful if we did not understand what performance issue we are facing here. In the previous example, in the event of setting the `inputX` siblings at the same time, Vinum would have to launch two updates, one after the other. The secondary update is considered wasteful here since an entire dependency tree recalculation was applied for the second time without a need.

Implementating a method for solving this issue can be broken down into two different parts:
1. Collect a group of state objects, and raw set their values
2. Launch a single update per unique dependency tree. This is considered off-topic, so just understand its some voodoo magic for determining what dependents we should update.

So you, as the good and considerate Maintainer, decided to implement a `batchSet` public function. This function is insanely simple, and what does it do is already outlined before.

And now, the performance issue that was found in the previous code example has been fixed:
```lua
     -- Niche details related to Vinum are omitted.
    local input1 = Hold(LastPosition)
    local input2 = Hold(CurrentPosition)

    local expensiveComputation = Compute(function(use)
        -- This is some very expensive computation!
        return findDistance(use(input1), use(input2))
    end)

    -- Now, expensiveComputation will only update once instead of twice!
    batchSet({input1, input2}, Point.new(0, 10, 0), Point.new(10, 0, 0))
```

# Perspective Change
*Insert some dramatic effects here*

Lets assume you are just a normal User of Vinum with the amazing new optimization method. 

Actually, you are not normal. You are an advocate of Batch processing, and probably world peace too.

So obviously, you would want all writes of all stuff to be batched to the next frame. I mean, how could you not do this- Vinum just added a new flexible and barebones `batchSet` method that will just help you do that without you needing to touch Vinum's Internals Hell.

So now, you, being the Advocate of Batch processing, you implemented a standardized `set` method to be used across all codebases, that secretly just add a some data to an internal data structure that is being read by a system like this:
```lua
    local ObjectsToSet = {} -- {object_1, object_2}
    local Values = {} -- {value_1, value_2}
    RunService.Stepped:Connect(function()
        batchSet(ObjectsToSet, table.unpack(Values))

        -- this is only for demonstraction purposes. In a real scenario, this could be optimized by a lot.
        table.clear(ObjectsToSet)
        table.clear(Values)
    end)
```

And now, all writes to every state object are batched, and will be applied at the next frame!

*Insert some weird noises here*

Of course, since you are a true Batch Processing follower, you immediately implemented it without thinking about the actual requirements of the said system.

And now, since any ***write request*** will be batched, your server systems are now "lagging" by one frame, and you have finally fallen to your enemy, that is Latency.


# When Batching is Fine

Of course, batching processing everything isn't fine, as this could easily introduce some sort of data latency. Its always ideal to only "batch-process" some parts of your overall transactions, ***especially*** in a context like state management. 